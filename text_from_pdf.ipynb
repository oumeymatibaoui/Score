{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f525d4a-7b3e-400d-8e2e-81dcb50c244c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymupdf in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (1.26.1)\n",
      "Requirement already satisfied: pytesseract in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (0.3.13)\n",
      "Requirement already satisfied: Pillow in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (11.0.0)\n",
      "Requirement already satisfied: langdetect in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (1.0.9)\n",
      "Requirement already satisfied: packaging>=21.3 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from pytesseract) (23.1)\n",
      "Requirement already satisfied: six in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from langdetect) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pymupdf pytesseract Pillow langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e36406d-94d6-4fdd-a327-b2eb2c9c4082",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-generativeai in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (0.8.5)\n",
      "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from google-generativeai) (0.6.15)\n",
      "Requirement already satisfied: google-api-core in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from google-generativeai) (2.25.1)\n",
      "Requirement already satisfied: google-api-python-client in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from google-generativeai) (2.173.0)\n",
      "Requirement already satisfied: google-auth>=2.15.0 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from google-generativeai) (2.40.3)\n",
      "Requirement already satisfied: protobuf in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from google-generativeai) (5.29.5)\n",
      "Requirement already satisfied: pydantic in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from google-generativeai) (2.11.7)\n",
      "Requirement already satisfied: tqdm in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from google-generativeai) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from google-generativeai) (4.14.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from google-api-core->google-generativeai) (1.70.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.18.0 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from google-api-core->google-generativeai) (2.32.4)\n",
      "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from pydantic->google-generativeai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from pydantic->google-generativeai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from pydantic->google-generativeai) (0.4.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from tqdm->google-generativeai) (0.4.6)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from google-api-core->google-generativeai) (1.73.0)\n",
      "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from google-api-core->google-generativeai) (1.71.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.0.9)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.4.8)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2025.4.26)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b33b1b2-6cf0-4933-bc9d-94063b485500",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymupdf in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (1.26.1)\n",
      "Requirement already satisfied: easyocr in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (1.7.2)\n",
      "Requirement already satisfied: langdetect in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (1.0.9)\n",
      "Requirement already satisfied: Pillow in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (11.0.0)\n",
      "Requirement already satisfied: torch in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from easyocr) (2.7.1)\n",
      "Requirement already satisfied: torchvision>=0.5 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from easyocr) (0.22.1)\n",
      "Requirement already satisfied: opencv-python-headless in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from easyocr) (4.11.0.86)\n",
      "Requirement already satisfied: scipy in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from easyocr) (1.15.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from easyocr) (2.3.1)\n",
      "Requirement already satisfied: scikit-image in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from easyocr) (0.20.0)\n",
      "Requirement already satisfied: python-bidi in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from easyocr) (0.6.6)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from easyocr) (6.0.2)\n",
      "Requirement already satisfied: Shapely in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from easyocr) (2.1.1)\n",
      "Requirement already satisfied: pyclipper in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from easyocr) (1.3.0.post6)\n",
      "Requirement already satisfied: ninja in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from easyocr) (1.11.1.4)\n",
      "Requirement already satisfied: six in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from langdetect) (1.16.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from torch->easyocr) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from torch->easyocr) (4.14.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from torch->easyocr) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from torch->easyocr) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from torch->easyocr) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from torch->easyocr) (2025.5.1)\n",
      "Requirement already satisfied: imageio>=2.4.1 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from scikit-image->easyocr) (2.26.0)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from scikit-image->easyocr) (2023.4.12)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from scikit-image->easyocr) (1.4.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from scikit-image->easyocr) (23.1)\n",
      "Requirement already satisfied: lazy_loader>=0.1 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from scikit-image->easyocr) (0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch->easyocr) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from jinja2->torch->easyocr) (2.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pymupdf easyocr langdetect Pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42d76679-f908-4b0f-96fb-a6f8998a0e66",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Langue détectée : English\n",
      "\n",
      "=== EXPERIENCE ===\n",
      "OUMEYMA TIBAOUI oumeimatibaoui gmail co m 2023 2024 ELITEZCOM Nabeul Developed a web application for invoicing and commercial operations using Angular frontend and Symfony backend Integrated an AI powered module using Python for generating data driven business insights Built a smart virtual assistant that interacts with the database in real time for data access and updates\n",
      "\n",
      "=== EDUCATION ===\n",
      "PROGRAMMING LANGUAGES FRAMEWORKS Python Java C PHP JavaScript TypeScript Dart Angular Symfony Spring Boot Flutter Bootstrap DATABASES FireBase MySQL SQL MongoDB SOFT\n",
      "\n",
      "=== SKILLS ===\n",
      "Teamwork Communication Time Management Problem Solving Organization PROJETS ACADEMIQUES LinkedIn FINAL YEAR INTERN ONLINE BILLING AND COMMERCIAL MANAGEMENT SYSTEM TECHNICAL SKILLS Arabic Native French Intermediate English Intermediate Turkish Beginner LANGUAGES 2023 ELITEZCOM Nabeul Designed and dev\n",
      "\n",
      "=== ACADEMIC PROJECTS ===\n",
      "Not found in the CV\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import fitz  # PyMuPDF\n",
    "from langdetect import detect\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "PDF_PATH = \"C:/Users/OUMAIMA/Downloads/cv_oumeymaFinal (1).pdf\"\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extract text from PDF using PyMuPDF.\"\"\"\n",
    "    text = \"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    for page in doc:\n",
    "        page_text = page.get_text()\n",
    "        if page_text.strip():\n",
    "            text += page_text + \"\\n\"\n",
    "    doc.close()\n",
    "    return text\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Remove special chars, bullet points, normalize spaces, and replace accented chars with non-accented equivalents.\"\"\"\n",
    "    # Replace accented characters with non-accented equivalents\n",
    "    text = text.replace('à', 'a').replace('â', 'a').replace('ä', 'a')\n",
    "    text = text.replace('é', 'e').replace('è', 'e').replace('ê', 'e').replace('ë', 'e')\n",
    "    text = text.replace('î', 'i').replace('ï', 'i')\n",
    "    text = text.replace('ô', 'o').replace('ö', 'o')\n",
    "    text = text.replace('ù', 'u').replace('û', 'u').replace('ü', 'u')\n",
    "    text = text.replace('ç', 'c')\n",
    "    # Remove remaining special characters, keeping only alphanumeric and spaces\n",
    "    text = re.sub(r'[^A-Za-z0-9\\s]', ' ', text)\n",
    "    # Normalize spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def detect_language(text):\n",
    "    \"\"\"Detect if the text is English or French.\"\"\"\n",
    "    try:\n",
    "        lang = detect(text[:500])\n",
    "        return \"French\" if lang == \"fr\" else \"English\" if lang == \"en\" else f\"Other ({lang})\"\n",
    "    except:\n",
    "        return \"Unknown\"\n",
    "\n",
    "def extract_sections_regex(raw_text, language):\n",
    "    \"\"\"Extract Experience, Education, Skills, and Projects using regex, based on language, before cleaning.\"\"\"\n",
    "    sections = {\n",
    "        \"experience\": \"Non trouvée dans le CV\",\n",
    "        \"education\": \"Non trouvée dans le CV\",\n",
    "        \"skills\": \"Non trouvée dans le CV\",\n",
    "        \"academic_projects\": \"Non trouvée dans le CV\"\n",
    "    }\n",
    "\n",
    "    # Define patterns based on language, headers followed by \\n or symbol (not space or .)\n",
    "    if language == \"French\":\n",
    "        exp_pattern = r'\\b(exp[ée]rience|emploi|exp[ée]rience professionnelle)\\b\\s*([:;\\-\\n#|])(.*?)(?=\\b(formation|[ée]ducation|comp[ée]tences|comp[ée]tences techniques|projets acad[ée]miques|projets)\\b|$)'\n",
    "        edu_pattern = r'\\b(formation|[ée]ducation)\\b\\s*([:;\\-\\n#|])(.*?)(?=\\b(comp[ée]tences|comp[ée]tences techniques|exp[ée]rience|emploi|exp[ée]rience professionnelle|projets acad[ée]miques|projets)\\b|$)'\n",
    "        skills_pattern = r'\\b(comp[ée]tences|comp[ée]tences techniques)\\b\\s*([:;\\-\\n#|])(.*?)(?=\\b(formation|[ée]ducation|exp[ée]rience|emploi|exp[ée]rience professionnelle|projets acad[ée]miques|projets)\\b|$)'\n",
    "        projects_pattern = r'\\b(projets acad[ée]miques|projets)\\b\\s*([:;\\-\\n#|])(.*?)(?=\\b(formation|[ée]ducation|comp[ée]tences|comp[ée]tences techniques|exp[ée]rience|emploi|exp[ée]rience professionnelle)\\b|$)'\n",
    "    else:  # English or Other\n",
    "        exp_pattern = r'\\b(experience|work history|professional experience)\\b\\s*([:;\\-\\n#|])(.*?)(?=\\b(education|academic background|skills|technical skills|academic projects|projects)\\b|$)'\n",
    "        edu_pattern = r'\\b(education|academic background)\\b\\s*([:;\\-\\n#|])(.*?)(?=\\b(skills|technical skills|experience|work history|professional experience|academic projects|projects)\\b|$)'\n",
    "        skills_pattern = r'\\b(skills|technical skills)\\b\\s*([:;\\-\\n#|])(.*?)(?=\\b(education|academic background|experience|work history|professional experience|academic projects|projects)\\b|$)'\n",
    "        projects_pattern = r'\\b(academic projects|projects)\\b\\s*([:;\\-\\n#|])(.*?)(?=\\b(education|academic background|skills|technical skills|experience|work history|professional experience)\\b|$)'\n",
    "\n",
    "    # Match sections\n",
    "    experience_match = re.search(exp_pattern, raw_text, re.IGNORECASE | re.DOTALL)\n",
    "    education_match = re.search(edu_pattern, raw_text, re.IGNORECASE | re.DOTALL)\n",
    "    skills_match = re.search(skills_pattern, raw_text, re.IGNORECASE | re.DOTALL)\n",
    "    projects_match = re.search(projects_pattern, raw_text, re.IGNORECASE | re.DOTALL)\n",
    "\n",
    "    # Assign matched content to sections and clean the extracted text\n",
    "    if experience_match:\n",
    "        sections[\"experience\"] = clean_text(experience_match.group(3).strip())\n",
    "    if education_match:\n",
    "        sections[\"education\"] = clean_text(education_match.group(3).strip())\n",
    "    if skills_match:\n",
    "        sections[\"skills\"] = clean_text(skills_match.group(3).strip())\n",
    "    if projects_match:\n",
    "        sections[\"academic_projects\"] = clean_text(projects_match.group(3).strip())\n",
    "\n",
    "    return sections\n",
    "\n",
    "# === MAIN ===\n",
    "raw_text = extract_text_from_pdf(PDF_PATH)\n",
    "language = detect_language(raw_text)  # Detect language on raw text\n",
    "\n",
    "# Define section headers and not found message based on detected language\n",
    "if language == \"French\":\n",
    "    headers = {\n",
    "        \"experience\": \"EXPÉRIENCE\",\n",
    "        \"education\": \"FORMATION\",\n",
    "        \"skills\": \"COMPÉTENCES\",\n",
    "        \"academic_projects\": \"PROJETS ACADÉMIQUES\"\n",
    "    }\n",
    "    not_found = \"Non trouvée dans le CV\"\n",
    "else:  # English or Other\n",
    "    headers = {\n",
    "        \"experience\": \"EXPERIENCE\",\n",
    "        \"education\": \"EDUCATION\",\n",
    "        \"skills\": \"SKILLS\",\n",
    "        \"academic_projects\": \"ACADEMIC PROJECTS\"\n",
    "    }\n",
    "    not_found = \"Not found in the CV\"\n",
    "\n",
    "# Extract sections using regex on raw text\n",
    "sections = extract_sections_regex(raw_text, language)\n",
    "\n",
    "# Update sections with language-appropriate \"not found\" message\n",
    "for key in sections:\n",
    "    if sections[key] == \"Non trouvée dans le CV\":\n",
    "        sections[key] = not_found\n",
    "\n",
    "# Print results\n",
    "print(f\"\\nLangue détectée : {language}\\n\")\n",
    "print(f\"=== {headers['experience']} ===\")\n",
    "print(sections[\"experience\"][:800] + \"\\n\")\n",
    "print(f\"=== {headers['education']} ===\")\n",
    "print(sections[\"education\"][:500] + \"\\n\")\n",
    "print(f\"=== {headers['skills']} ===\")\n",
    "print(sections[\"skills\"][:300] + \"\\n\")\n",
    "print(f\"=== {headers['academic_projects']} ===\")\n",
    "print(sections[\"academic_projects\"][:300] + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cedfa9e3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (3.8.7)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from spacy) (1.0.13)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from spacy) (3.0.10)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from spacy) (8.3.6)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from spacy) (0.16.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from spacy) (4.65.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from spacy) (2.3.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from spacy) (2.32.4)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from spacy) (2.11.7)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from spacy) (68.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from spacy) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.14.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.4.26)\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.0.4)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (14.0.0)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.1)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (5.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.1.1)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999560ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d6c4f7-6a1b-44c5-8e9c-d5e07bdc821c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip uninstall numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f17728-5167-43fa-b209-66ae89a49ccc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0341b7c0-4505-444a-a011-a11c3b70ca01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import fitz\n",
    "import spacy\n",
    "from langdetect import detect\n",
    "import io\n",
    "\n",
    "def extract_text(resume, ext):\n",
    "    \"\"\"Extract text from a PDF file.\"\"\"\n",
    "    if ext.lower() != \".pdf\":\n",
    "        raise ValueError(\"Only PDF files are supported\")\n",
    "    \n",
    "    text = \"\"\n",
    "    try:\n",
    "        if isinstance(resume, io.BytesIO):\n",
    "            doc = fitz.open(stream=resume, filetype=\"pdf\")\n",
    "        else:\n",
    "            doc = fitz.open(resume)\n",
    "        \n",
    "        for page in doc:\n",
    "            page_text = page.get_text()\n",
    "            if page_text.strip():\n",
    "                text += page_text + \"\\n\"\n",
    "        doc.close()\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Failed to extract text: {e}\")\n",
    "    return text\n",
    "\n",
    "def detect_language(text):\n",
    "    \"\"\"Detect if the text is English or French.\"\"\"\n",
    "    try:\n",
    "        lang = detect(text[:500])\n",
    "        return \"French\" if lang == \"fr\" else \"English\" if lang == \"en\" else f\"Other ({lang})\"\n",
    "    except:\n",
    "        return \"Unknown\"\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean text by removing special chars and normalizing spaces.\"\"\"\n",
    "    # Replace accented characters\n",
    "    text = text.replace('à', 'a').replace('â', 'a').replace('ä', 'a')\n",
    "    text = text.replace('é', 'e').replace('è', 'e').replace('ê', 'e').replace('ë', 'e')\n",
    "    text = text.replace('î', 'i').replace('ï', 'i')\n",
    "    text = text.replace('ô', 'o').replace('ö', 'o')\n",
    "    text = text.replace('ù', 'u').replace('û', 'u').replace('ü', 'u')\n",
    "    text = text.replace('ç', 'c')\n",
    "    # Remove special characters, keep alphanumeric and spaces\n",
    "    text = re.sub(r'[^A-Za-z0-9\\s]', ' ', text)\n",
    "    # Normalize spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def extract_name(nlp_doc, matcher):\n",
    "    \"\"\"Extract name using spaCy NER.\"\"\"\n",
    "    for ent in nlp_doc.ents:\n",
    "        if ent.label_ == \"PERSON\":\n",
    "            return ent.text\n",
    "    return None\n",
    "\n",
    "def extract_email(text):\n",
    "    \"\"\"Extract email address using regex.\"\"\"\n",
    "    match = re.search(r'[\\w\\.-]+@[\\w\\.-]+', text)\n",
    "    return match.group(0) if match else None\n",
    "\n",
    "def extract_mobile_number(text, custom_regex=None):\n",
    "    \"\"\"Extract phone number using regex.\"\"\"\n",
    "    if custom_regex:\n",
    "        match = re.search(custom_regex, text)\n",
    "    else:\n",
    "        match = re.search(r'(\\+?\\d{2,4}[-.\\s]?)?(\\(?\\d{2,4}\\)?[-.\\s]?)?(\\d{2,4}[-.\\s]?){2,4}', text)\n",
    "    return match.group(0) if match else None\n",
    "\n",
    "def extract_sections_regex(raw_text, language):\n",
    "    \"\"\"Extract Experience, Education, Skills, and Projects using regex.\"\"\"\n",
    "    sections = {\n",
    "        \"experience\": None,\n",
    "        \"education\": None,\n",
    "        \"skills\": None,\n",
    "        \"academic_projects\": None\n",
    "    }\n",
    "\n",
    "    # Define patterns based on language\n",
    "    if language == \"French\":\n",
    "        exp_pattern = r'\\b(exp[ée]rience|emploi|exp[ée]rience professionnelle)\\b\\s*([:;\\-\\n#|])(.*?)(?=\\b(formation|[ée]ducation|comp[ée]tences|comp[ée]tences techniques|projets acad[ée]miques|projets)\\b|$)'\n",
    "        edu_pattern = r'\\b(formation|[ée]ducation)\\b\\s*([:;\\-\\n#|])(.*?)(?=\\b(comp[ée]tences|comp[ée]tences techniques|exp[ée]rience|emploi|exp[ée]rience professionnelle|projets acad[ée]miques|projets)\\b|$)'\n",
    "        skills_pattern = r'\\b(comp[ée]tences|comp[ée]tences techniques)\\b\\s*([:;\\-\\n#|])(.*?)(?=\\b(formation|[ée]ducation|exp[ée]rience|emploi|exp[ée]rience professionnelle|projets acad[ée]miques|projets)\\b|$)'\n",
    "        projects_pattern = r'\\b(projets acad[ée]miques|projets)\\b\\s*([:;\\-\\n#|])(.*?)(?=\\b(formation|[ée]ducation|comp[ée]tences|comp[ée]tences techniques|exp[ée]rience|emploi|exp[ée]rience professionnelle)\\b|$)'\n",
    "        not_found = \"Non trouvée dans le CV\"\n",
    "    else:\n",
    "        exp_pattern = r'\\b(experience|work history|professional experience)\\b\\s*([:;\\-\\n#|])(.*?)(?=\\b(education|academic background|skills|technical skills|academic projects|projects)\\b|$)'\n",
    "        edu_pattern = r'\\b(education|academic background)\\b\\s*([:;\\-\\n#|])(.*?)(?=\\b(skills|technical skills|experience|work history|professional experience|academic projects|projects)\\b|$)'\n",
    "        skills_pattern = r'\\b(skills|technical skills)\\b\\s*([:;\\-\\n#|])(.*?)(?=\\b(education|academic background|experience|work history|professional experience|academic projects|projects)\\b|$)'\n",
    "        projects_pattern = r'\\b(academic projects|projects)\\b\\s*([:;\\-\\n#|])(.*?)(?=\\b(education|academic background|skills|technical skills|experience|work history|professional experience)\\b|$)'\n",
    "        not_found = \"Not found in the CV\"\n",
    "\n",
    "    # Extract sections\n",
    "    exp_match = re.search(exp_pattern, raw_text, re.IGNORECASE | re.DOTALL)\n",
    "    edu_match = re.search(edu_pattern, raw_text, re.IGNORECASE | re.DOTALL)\n",
    "    skills_match = re.search(skills_pattern, raw_text, re.IGNORECASE | re.DOTALL)\n",
    "    projects_match = re.search(projects_pattern, raw_text, re.IGNORECASE | re.DOTALL)\n",
    "\n",
    "    # Assign cleaned text to sections\n",
    "    if exp_match:\n",
    "        sections[\"experience\"] = clean_text(exp_match.group(3).strip())\n",
    "    else:\n",
    "        sections[\"experience\"] = not_found\n",
    "    if edu_match:\n",
    "        sections[\"education\"] = clean_text(edu_match.group(3).strip())\n",
    "    else:\n",
    "        sections[\"education\"] = not_found\n",
    "    if skills_match:\n",
    "        sections[\"skills\"] = clean_text(skills_match.group(3).strip())\n",
    "    else:\n",
    "        sections[\"skills\"] = not_found\n",
    "    if projects_match:\n",
    "        sections[\"academic_projects\"] = clean_text(projects_match.group(3).strip())\n",
    "    else:\n",
    "        sections[\"academic_projects\"] = not_found\n",
    "\n",
    "    return sections\n",
    "\n",
    "def get_number_of_pages(resume, ext):\n",
    "    \"\"\"Get the number of pages in the PDF.\"\"\"\n",
    "    if ext.lower() != \".pdf\":\n",
    "        return None\n",
    "    try:\n",
    "        if isinstance(resume, io.BytesIO):\n",
    "            doc = fitz.open(stream=resume, filetype=\"pdf\")\n",
    "        else:\n",
    "            doc = fitz.open(resume)\n",
    "        pages = doc.page_count\n",
    "        doc.close()\n",
    "        return pages\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a915c045-ebe9-4be7-a97d-83f12894bfb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d6198d-66af-4d1b-a8c0-ac88907bce55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.data import Sentence\n",
    "from flair.embeddings import TransformerDocumentEmbeddings\n",
    "from flair.similarity import DocumentSimilarity\n",
    "\n",
    "# Mock setup (replace with actual flair initialization)\n",
    "# embedding = TransformerDocumentEmbeddings('distilbert-base-uncased')\n",
    "# similarity = DocumentSimilarity(embedding)\n",
    "\n",
    "def mock_flair_similarity(text1, text2):\n",
    "    # Placeholder for flair similarity; use keyword overlap as a proxy\n",
    "    standard_keywords = [\"worked\", \"developed\", \"experience\", \"team\", \"project\"]\n",
    "    text1_words = text1.lower().split()\n",
    "    text2_words = text2.lower().split()\n",
    "    common_words = set(text1_words) & set(text2_words) & set(standard_keywords)\n",
    "    return len(common_words) / len(standard_keywords)  # Normalized score [0, 1]\n",
    "\n",
    "def find_best_experience_block(lines, standard_experience):\n",
    "    if len(lines) < 5:\n",
    "        return \"CV too short for 5-line window.\"\n",
    "\n",
    "    # Standard experience description\n",
    "    standard_desc = standard_experience.lower()\n",
    "\n",
    "    # Initialize sliding window\n",
    "    window_size = 5\n",
    "    best_start = 0\n",
    "    best_end = window_size\n",
    "    max_score = 0.0\n",
    "    current_start = 0\n",
    "\n",
    "    # Slide 5-line window across CV\n",
    "    while current_start <= len(lines) - window_size:\n",
    "        window = \"\\n\".join(lines[current_start:current_start + window_size])\n",
    "        score = mock_flair_similarity(window, standard_desc)\n",
    "        if score > max_score:\n",
    "            max_score = score\n",
    "            best_start = current_start\n",
    "            best_end = current_start + window_size\n",
    "        current_start += 1\n",
    "\n",
    "    # Optimize window size around best 5-line block\n",
    "    best_block = \"\\n\".join(lines[best_start:best_end])\n",
    "    current_block = best_block\n",
    "    current_score = max_score\n",
    "    min_lines = 1\n",
    "    max_lines = len(lines)\n",
    "\n",
    "    # Expand or contract to find optimal length\n",
    "    while True:\n",
    "        # Try adding a line\n",
    "        if best_end < len(lines):\n",
    "            new_end = best_end + 1\n",
    "            new_block = \"\\n\".join(lines[best_start:new_end])\n",
    "            new_score = mock_flair_similarity(new_block, standard_desc)\n",
    "            if new_score > current_score:\n",
    "                current_block = new_block\n",
    "                current_score = new_score\n",
    "                best_end = new_end\n",
    "            else:\n",
    "                break\n",
    "        # Try removing a line from start if more than 5 lines\n",
    "        elif best_start < best_end - 1:\n",
    "            new_start = best_start + 1\n",
    "            new_block = \"\\n\".join(lines[new_start:best_end])\n",
    "            new_score = mock_flair_similarity(new_block, standard_desc)\n",
    "            if new_score >= current_score:\n",
    "                current_block = new_block\n",
    "                current_score = new_score\n",
    "                best_start = new_start\n",
    "            else:\n",
    "                break\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return current_block, best_end - best_start\n",
    "\n",
    "# Example usage\n",
    "cv_text = \"\"\"Passionate and dedicated software engineer with a strong foundation in computer science. I enjoy creating innovative and practical\n",
    "solutions across desktop, web, and mobile platforms. I am eager to learn, proactive in my approach, and constantly seeking new challenges\n",
    "where I can apply both my technical and soft skills to contribute meaningfully to any team or project.\n",
    "+216 23 577 840\n",
    "Tunisian | Dar Chaaben El Fehri, Nabeul\n",
    "WORK EXPERIENCE\n",
    "OUMEYMA TIBAOUI\n",
    "oumeimatibaoui@gmail.co\n",
    "m\n",
    "2023–2024 | ELITEZCOM, Nabeul\n",
    "• Developed a web application for invoicing and commercial operations using Angular (frontend) and Symfony (backend).\n",
    "• Integrated an AI-powered module using Python for generating data-driven business insights.\n",
    "• Built a smart virtual assistant that interacts with the database in real-time for data access and updates.\n",
    "EDUCATION\n",
    "PROGRAMMING LANGUAGES & FRAMEWORKS:\n",
    " • Python – Java – C++ – PHP – JavaScript – TypeScript – Dart\"\"\"\n",
    "lines = cv_text.splitlines()\n",
    "standard_experience = \"I worked as a software engineer developing web applications and collaborated with teams.\"\n",
    "best_block, line_count = find_best_experience_block(lines, standard_experience)\n",
    "print(f\"Best experience block:\\n{best_block}\")\n",
    "print(f\"Number of lines: {line_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064bb7d2-3741-42b9-8e8c-11119f321409",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"hello!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6d037d0-4939-4c9a-a65d-036a8f5452ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: flair in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (0.15.1)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: boto3>=1.20.27 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from flair) (1.38.41)\n",
      "Requirement already satisfied: conllu<5.0.0,>=4.0 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from flair) (4.5.3)\n",
      "Requirement already satisfied: deprecated>=1.2.13 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from flair) (1.2.18)\n",
      "Requirement already satisfied: ftfy>=6.1.0 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from flair) (6.3.1)\n",
      "Requirement already satisfied: gdown>=4.4.0 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from flair) (5.2.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.10.0 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from flair) (0.33.0)\n",
      "Requirement already satisfied: langdetect>=1.0.9 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from flair) (1.0.9)\n",
      "Requirement already satisfied: lxml>=4.8.0 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from flair) (4.9.3)\n",
      "Requirement already satisfied: matplotlib>=2.2.3 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from flair) (3.7.2)\n",
      "Requirement already satisfied: more-itertools>=8.13.0 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from flair) (10.3.0)\n",
      "Requirement already satisfied: mpld3>=0.3 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from flair) (0.5.10)\n",
      "Requirement already satisfied: pptree>=3.1 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from flair) (3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from flair) (2.8.2)\n",
      "Requirement already satisfied: pytorch-revgrad>=0.2.0 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from flair) (0.2.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from flair) (2022.7.9)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from flair) (1.3.0)\n",
      "Requirement already satisfied: segtok>=1.5.11 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from flair) (1.5.11)\n",
      "Requirement already satisfied: sqlitedict>=2.0.0 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from flair) (2.1.0)\n",
      "Requirement already satisfied: tabulate>=0.8.10 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from flair) (0.8.10)\n",
      "Requirement already satisfied: torch>=1.13.1 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from flair) (2.7.1)\n",
      "Requirement already satisfied: tqdm>=4.63.0 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from flair) (4.65.0)\n",
      "Requirement already satisfied: transformer-smaller-training-vocab>=0.2.3 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from flair) (0.4.2)\n",
      "Requirement already satisfied: transformers[sentencepiece]<5.0.0,>=4.25.0 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from flair) (4.32.1)\n",
      "Requirement already satisfied: wikipedia-api>=0.5.7 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from flair) (0.8.1)\n",
      "Requirement already satisfied: bioc<3.0.0,>=2.0.0 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from flair) (2.1)\n",
      "Requirement already satisfied: jsonlines>=1.2.0 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from bioc<3.0.0,>=2.0.0->flair) (4.0.0)\n",
      "Requirement already satisfied: intervaltree in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from bioc<3.0.0,>=2.0.0->flair) (3.1.0)\n",
      "Requirement already satisfied: docopt in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from bioc<3.0.0,>=2.0.0->flair) (0.6.2)\n",
      "Requirement already satisfied: botocore<1.39.0,>=1.38.41 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from boto3>=1.20.27->flair) (1.38.41)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from boto3>=1.20.27->flair) (0.10.0)\n",
      "Requirement already satisfied: s3transfer<0.14.0,>=0.13.0 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from boto3>=1.20.27->flair) (0.13.0)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from deprecated>=1.2.13->flair) (1.14.1)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from ftfy>=6.1.0->flair) (0.2.5)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from gdown>=4.4.0->flair) (4.12.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from gdown>=4.4.0->flair) (3.9.0)\n",
      "Requirement already satisfied: requests[socks] in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from gdown>=4.4.0->flair) (2.32.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.10.0->flair) (2025.5.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.10.0->flair) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.10.0->flair) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.10.0->flair) (4.14.0)\n",
      "Requirement already satisfied: six in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from langdetect>=1.0.9->flair) (1.16.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from matplotlib>=2.2.3->flair) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from matplotlib>=2.2.3->flair) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from matplotlib>=2.2.3->flair) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from matplotlib>=2.2.3->flair) (1.4.4)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from matplotlib>=2.2.3->flair) (1.24.3)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from matplotlib>=2.2.3->flair) (11.0.0)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from matplotlib>=2.2.3->flair) (3.0.9)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from mpld3>=0.3->flair) (3.1.2)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from scikit-learn>=1.0.2->flair) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from scikit-learn>=1.0.2->flair) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from scikit-learn>=1.0.2->flair) (2.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from torch>=1.13.1->flair) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from torch>=1.13.1->flair) (3.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from tqdm>=4.63.0->flair) (0.4.6)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from transformers[sentencepiece]<5.0.0,>=4.25.0->flair) (0.13.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from transformers[sentencepiece]<5.0.0,>=4.25.0->flair) (0.5.3)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from transformers[sentencepiece]<5.0.0,>=4.25.0->flair) (0.2.0)\n",
      "Requirement already satisfied: protobuf in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from transformers[sentencepiece]<5.0.0,>=4.25.0->flair) (5.29.5)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from botocore<1.39.0,>=1.38.41->boto3>=1.20.27->flair) (1.26.16)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from jsonlines>=1.2.0->bioc<3.0.0,>=2.0.0->flair) (22.1.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch>=1.13.1->flair) (1.3.0)\n",
      "Requirement already satisfied: accelerate>=0.20.3 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from transformers[sentencepiece]<5.0.0,>=4.25.0->flair) (1.8.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from beautifulsoup4->gdown>=4.4.0->flair) (2.4)\n",
      "Requirement already satisfied: sortedcontainers<3.0,>=2.0 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from intervaltree->bioc<3.0.0,>=2.0.0->flair) (2.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from jinja2->mpld3>=0.3->flair) (2.1.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from requests[socks]->gdown>=4.4.0->flair) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from requests[socks]->gdown>=4.4.0->flair) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from requests[socks]->gdown>=4.4.0->flair) (2025.4.26)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from requests[socks]->gdown>=4.4.0->flair) (1.7.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\oumaima\\anaconda3\\lib\\site-packages (from accelerate>=0.20.3->transformers[sentencepiece]<5.0.0,>=4.25.0->flair) (5.9.0)\n"
     ]
    }
   ],
   "source": [
    "pip install flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7ea7e3a-b9fd-4336-ab24-5b8c54d4533a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\n\nIMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!\n\nImporting the numpy C-extensions failed. This error can happen for\nmany reasons, often due to issues with your setup or how NumPy was\ninstalled.\n\nWe have compiled some common reasons and troubleshooting tips at:\n\n    https://numpy.org/devdocs/user/troubleshooting-importerror.html\n\nPlease note and check the following:\n\n  * The Python version is: Python3.11 from \"C:\\Users\\OUMAIMA\\anaconda3\\python.exe\"\n  * The NumPy version is: \"1.24.3\"\n\nand make sure that they are the versions you expect.\nPlease carefully study the documentation linked above for further help.\n\nOriginal error was: DLL load failed while importing _multiarray_umath: Le module spécifié est introuvable.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\numpy\\_core\\__init__.py:22\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 22\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m multiarray\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\numpy\\_core\\multiarray.py:11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfunctools\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _multiarray_umath, overrides\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_multiarray_umath\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing _multiarray_umath: Le module spécifié est introuvable.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfitz\u001b[39;00m  \u001b[38;5;66;03m# PyMuPDF\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangdetect\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m detect\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mflair\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membeddings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TransformerDocumentEmbeddings\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mflair\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sentence\n\u001b[0;32m      7\u001b[0m PDF_PATH \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:/Users/OUMAIMA/Downloads/cv_oumeymaFinal (1).pdf\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\flair\\__init__.py:5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpathlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m set_seed \u001b[38;5;28;01mas\u001b[39;00m hf_set_seed\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# global variable: cache_root\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\__init__.py:1816\u001b[0m\n\u001b[0;32m   1813\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_tensor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tensor  \u001b[38;5;66;03m# usort: skip\u001b[39;00m\n\u001b[0;32m   1815\u001b[0m \u001b[38;5;66;03m# needs to be after torch.Tensor is defined to avoid circular dependencies\u001b[39;00m\n\u001b[1;32m-> 1816\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m storage \u001b[38;5;28;01mas\u001b[39;00m storage  \u001b[38;5;66;03m# usort: skip\u001b[39;00m\n\u001b[0;32m   1817\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstorage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m   1818\u001b[0m     _LegacyStorage,\n\u001b[0;32m   1819\u001b[0m     _StorageBase,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1822\u001b[0m     UntypedStorage,\n\u001b[0;32m   1823\u001b[0m )\n\u001b[0;32m   1826\u001b[0m \u001b[38;5;66;03m# NOTE: New <type>Storage classes should never be added. When adding a new\u001b[39;00m\n\u001b[0;32m   1827\u001b[0m \u001b[38;5;66;03m# dtype, use torch.storage.TypedStorage directly.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\storage.py:27\u001b[0m\n\u001b[0;32m     23\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTypedStorage\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUntypedStorage\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 27\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m     29\u001b[0m     HAS_NUMPY \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\numpy\\__init__.py:153\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m polynomial\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m random\n\u001b[1;32m--> 153\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ctypeslib\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ma\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m matrixlib \u001b[38;5;28;01mas\u001b[39;00m _mat\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\numpy\\ctypeslib\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_ctypeslib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      2\u001b[0m     __all__,\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;18m__doc__\u001b[39m,\n\u001b[0;32m      4\u001b[0m     _concrete_ndptr,\n\u001b[0;32m      5\u001b[0m     _ndptr,\n\u001b[0;32m      6\u001b[0m     as_array,\n\u001b[0;32m      7\u001b[0m     as_ctypes,\n\u001b[0;32m      8\u001b[0m     as_ctypes_type,\n\u001b[0;32m      9\u001b[0m     c_intp,\n\u001b[0;32m     10\u001b[0m     ctypes,\n\u001b[0;32m     11\u001b[0m     load_library,\n\u001b[0;32m     12\u001b[0m     ndpointer,\n\u001b[0;32m     13\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\numpy\\ctypeslib\\_ctypeslib.py:58\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m---> 58\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmultiarray\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmu\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m set_module\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\numpy\\_core\\__init__.py:48\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[0;32m     25\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     26\u001b[0m \n\u001b[0;32m     27\u001b[0m \u001b[38;5;124mIMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m \u001b[38;5;241m%\u001b[39m (sys\u001b[38;5;241m.\u001b[39mversion_info[\u001b[38;5;241m0\u001b[39m], sys\u001b[38;5;241m.\u001b[39mversion_info[\u001b[38;5;241m1\u001b[39m], sys\u001b[38;5;241m.\u001b[39mexecutable,\n\u001b[0;32m     47\u001b[0m         __version__, exc)\n\u001b[1;32m---> 48\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m envkey \u001b[38;5;129;01min\u001b[39;00m env_added:\n",
      "\u001b[1;31mImportError\u001b[0m: \n\nIMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!\n\nImporting the numpy C-extensions failed. This error can happen for\nmany reasons, often due to issues with your setup or how NumPy was\ninstalled.\n\nWe have compiled some common reasons and troubleshooting tips at:\n\n    https://numpy.org/devdocs/user/troubleshooting-importerror.html\n\nPlease note and check the following:\n\n  * The Python version is: Python3.11 from \"C:\\Users\\OUMAIMA\\anaconda3\\python.exe\"\n  * The NumPy version is: \"1.24.3\"\n\nand make sure that they are the versions you expect.\nPlease carefully study the documentation linked above for further help.\n\nOriginal error was: DLL load failed while importing _multiarray_umath: Le module spécifié est introuvable.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import fitz  # PyMuPDF\n",
    "from langdetect import detect\n",
    "from flair.embeddings import TransformerDocumentEmbeddings\n",
    "from flair.data import Sentence\n",
    "\n",
    "PDF_PATH = \"C:/Users/OUMAIMA/Downloads/cv_oumeymaFinal (1).pdf\"\n",
    "# Charger un modèle transformer de Flair (comme BERT)\n",
    "embedding_model = TransformerDocumentEmbeddings('bert-base-uncased')\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extract text from PDF using PyMuPDF.\"\"\"\n",
    "    text = \"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    for page in doc:\n",
    "        page_text = page.get_text()\n",
    "        if page_text.strip():\n",
    "            text += page_text + \"\\n\"\n",
    "    doc.close()\n",
    "    return text\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean text by replacing accented characters and removing special chars except \\n and spaces.\"\"\"\n",
    "    text = text.replace('à', 'a').replace('â', 'a').replace('ä', 'a')\n",
    "    text = text.replace('é', 'e').replace('è', 'e').replace('ê', 'e').replace('ë', 'e')\n",
    "    text = text.replace('î', 'i').replace('ï', 'i')\n",
    "    text = text.replace('ô', 'o').replace('ö', 'o')\n",
    "    text = text.replace('ù', 'u').replace('û', 'u').replace('ü', 'u')\n",
    "    text = text.replace('ç', 'c')\n",
    "    text = re.sub(r'[^\\w\\s\\n]', '', text)\n",
    "    return text\n",
    "\n",
    "def detect_language(text):\n",
    "    \"\"\"Detect if the text is English or French with a larger sample and header check.\"\"\"\n",
    "    try:\n",
    "        sample = text[:1000].lower()\n",
    "        lang = detect(sample)\n",
    "        if re.search(r'\\b(formation|éducation|compétences|projets académiques)\\b', sample):\n",
    "            return \"French\"\n",
    "        return \"French\" if lang == \"fr\" else \"English\" if lang == \"en\" else f\"Other ({lang})\"\n",
    "    except:\n",
    "        return \"Unknown\"\n",
    "\n",
    "def mock_flair_similarity(text1, text2):\n",
    "    \"\"\"Placeholder for flair similarity; use keyword overlap as a proxy with section-specific keywords.\"\"\"\n",
    "    # Define keywords for each section type\n",
    "    section_keywords = {\n",
    "        \"experience\": [\"worked\", \"developed\", \"experience\", \"team\", \"project\", \"intern\", \"built\"],\n",
    "        \"skills\": [\"skills\", \"languages\", \"frameworks\", \"proficient\", \"tools\", \"databases\"],\n",
    "        \"education\": [\"education\", \"degree\", \"graduated\", \"school\", \"university\", \"engineering\"]\n",
    "    }\n",
    "    # Use the appropriate keyword set based on section_type\n",
    "    keywords = section_keywords.get(section_type, section_keywords[\"experience\"])\n",
    "    text1_words = text1.lower().split()\n",
    "    text2_words = text2.lower().split()\n",
    "    common_words = set(text1_words) & set(text2_words) & set(keywords)\n",
    "    score = len(common_words) / len(keywords) if keywords else 0.0  # Normalized score [0, 1]\n",
    "    print(f\"Text1 words: {text1_words[:5]}... Text2 words: {text2_words[:5]}... Common: {common_words} (Keywords: {keywords})\")\n",
    "    return score\n",
    "def flair_similarity(text1, text2):\n",
    "    \"\"\"Compare deux textes en utilisant des embeddings contextuels Flair.\"\"\"\n",
    "    sentence1 = Sentence(text1)\n",
    "    sentence2 = Sentence(text2)\n",
    "\n",
    "    # Générer les embeddings\n",
    "    embedding_model.embed(sentence1)\n",
    "    embedding_model.embed(sentence2)\n",
    "\n",
    "    # Extraire les vecteurs\n",
    "    vec1 = sentence1.embedding.detach().numpy()\n",
    "    vec2 = sentence2.embedding.detach().numpy()\n",
    "\n",
    "    # Cosine similarity entre les vecteurs\n",
    "    score = np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "\n",
    "    return float(score)  # score entre -1 et 1\n",
    "def find_best_section_block(lines, standard_description, section_type=\"experience\"):\n",
    "    \"\"\"Find the best block for a given section (experience, skills, or education) by sliding a 5-line window and optimizing.\"\"\"\n",
    "    if len(lines) < 5:\n",
    "        return \"CV too short for 5-line window.\"\n",
    "\n",
    "    # Standard description for the specific section\n",
    "    standard_desc = standard_description.lower()\n",
    "\n",
    "    # Initialize sliding window\n",
    "    window_size = 5\n",
    "    best_start = 0\n",
    "    best_end = window_size\n",
    "    max_score = 0.0\n",
    "    current_start = 0\n",
    "\n",
    "    # Slide 5-line window across CV with debugging\n",
    "    print(f\"\\n=== SLIDING WINDOW PROCESS ({section_type.capitalize()}) ===\")\n",
    "    while current_start <= len(lines) - window_size:\n",
    "        window = \"\\n\".join(lines[current_start:current_start + window_size])\n",
    "        score = flair_similarity(window, standard_desc)\n",
    "        print(f\"Window {current_start}-{current_start + window_size}: {window[:50]}... (Score: {score:.2f})\")\n",
    "        if score > max_score:\n",
    "            max_score = score\n",
    "            best_start = current_start\n",
    "            best_end = current_start + window_size\n",
    "            print(f\"  -> New best window found! Score: {max_score:.2f}\")\n",
    "        current_start += 1\n",
    "\n",
    "    # Optimize window size around best 5-line block\n",
    "    best_block = \"\\n\".join(lines[best_start:best_end])\n",
    "    current_block = best_block\n",
    "    current_score = max_score\n",
    "    min_lines = 1\n",
    "    max_lines = len(lines)\n",
    "\n",
    "    print(f\"\\n=== WINDOW OPTIMIZATION ({section_type.capitalize()}) ===\")\n",
    "    print(f\"Initial best block ({best_end - best_start} lines):\\n{best_block}\")\n",
    "\n",
    "    # Expand or contract to find optimal length\n",
    "    while True:\n",
    "        # Try adding a line\n",
    "        if best_end < len(lines):\n",
    "            new_end = best_end + 1\n",
    "            new_block = \"\\n\".join(lines[best_start:new_end])\n",
    "            new_score = flair_similarity(new_block, standard_desc)\n",
    "            print(f\"Trying +1 line ({new_end - best_start} lines): {new_block[:50]}... (Score: {new_score:.2f})\")\n",
    "            if new_score > current_score:\n",
    "                current_block = new_block\n",
    "                current_score = new_score\n",
    "                best_end = new_end\n",
    "                print(f\"  -> Improved! New score: {current_score:.2f}\")\n",
    "            else:\n",
    "                print(f\"  -> No improvement, stopping expansion.\")\n",
    "                break\n",
    "        # Try removing a line from start if more than 5 lines\n",
    "        elif best_start < best_end - 1:\n",
    "            new_start = best_start + 1\n",
    "            new_block = \"\\n\".join(lines[new_start:best_end])\n",
    "            new_score = flair_similarity(new_block, standard_desc)\n",
    "            print(f\"Trying -1 line ({best_end - new_start} lines): {new_block[:50]}... (Score: {new_score:.2f})\")\n",
    "            if new_score >= current_score:\n",
    "                current_block = new_block\n",
    "                current_score = new_score\n",
    "                best_start = new_start\n",
    "                print(f\"  -> Improved! New score: {current_score:.2f}\")\n",
    "            else:\n",
    "                print(f\"  -> No improvement, stopping contraction.\")\n",
    "                break\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return current_block, best_end - best_start\n",
    "\n",
    "# MAIN\n",
    "raw_text = extract_text_from_pdf(PDF_PATH)\n",
    "cleaned_text = clean_text(raw_text)\n",
    "lines = cleaned_text.splitlines()  # Split into lines here\n",
    "language = detect_language(raw_text)\n",
    "print(f\"\\n📄 Langue détectée : {language}\")\n",
    "\n",
    "# Standard descriptions for each section\n",
    "standard_experience = \"Completed a 4-month internship at TechSolutions where I developed a RESTful API using Node.js and MongoDB, improving data processing.\"\n",
    "standard_skills = \"Proficient in Python, Java, JavaScript, and frameworks like Angular, with experience in databases such as MySQL and Firebase.\"\n",
    "standard_education = \"Graduated with a Bachelor’s Degree in Computer Science from a university in 2023.\"\n",
    "\n",
    "# Detect blocks for each section\n",
    "best_experience_block, experience_line_count = find_best_section_block(lines, standard_experience, \"experience\")\n",
    "best_skills_block, skills_line_count = find_best_section_block(lines, standard_skills, \"skills\")\n",
    "best_education_block, education_line_count = find_best_section_block(lines, standard_education, \"education\")\n",
    "\n",
    "# Print results\n",
    "print(\"\\n=== SECTION D'EXPÉRIENCE DÉTECTÉE ===\")\n",
    "print(f\"Best experience block:\\n{best_experience_block}\")\n",
    "print(f\"Number of lines: {experience_line_count}\")\n",
    "\n",
    "print(\"\\n=== SECTION DES COMPÉTENCES DÉTECTÉE ===\")\n",
    "print(f\"Best skills block:\\n{best_skills_block}\")\n",
    "print(f\"Number of lines: {skills_line_count}\")\n",
    "\n",
    "print(\"\\n=== SECTION D'ÉDUCATION DÉTECTÉE ===\")\n",
    "print(f\"Best education block:\\n{best_education_block}\")\n",
    "print(f\"Number of lines: {education_line_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046b96f0-6d9d-4803-8c7a-4c920fccc905",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
