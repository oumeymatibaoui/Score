import re
from flair.embeddings import TransformerDocumentEmbeddings
from flair.data import Sentence
from langdetect import detect
import torch
from torch.nn.functional import cosine_similarity
import pdfplumber
import pytesseract
from PIL import Image
from textblob import TextBlob
from pdf2image import convert_from_path
import layoutparser as lp

# Set Tesseract path for Windows
pytesseract.pytesseract.tesseract_cmd = r'C:\Program Files\Tesseract-OCR\tesseract.exe'

class PdfToText:
    def __init__(self, pdf_path, text_cv, embedding_model='distilbert-base-uncased'):
        self.pdf_path = pdf_path
        self.text_cv = text_cv
        try:
            self.embedding_model = TransformerDocumentEmbeddings(embedding_model)
        except Exception as e:
            print(f"Error initializing embedding model: {e}")
            raise
        self.keywords = {
            "experience": r"(experience|work experience|professional experience|job history|employment history|parcours professionnel|historique d'emploi|antécédents professionnels|stage / internship|stage|stages|internship|freelance|missions)",
            "education": r"^(education|diploma|degrees|certification|formation|études|étude|educational background|diplomas|certifications|parcours académique|études supérieures)\b",
            "skills": r"""
                \b(
                    python|java|c\+\+|c#|typescript|javascript|html|css|php|go|dart|scala|swift|r|
                    sql|mysql|mongodb|postgresql|oracle|nysql| Customer Service
                    react|angular|vue\.js|django|flask|spring|express|node\.js|laravel|
                    docker|
                    git|github|gitlab|bitbucket|
                    figma|adobe\s?xd|photoshop|illustrator|
                    agile|scrum|kanban|waterfall|jira|trello|notion|
                    linux|windows|mac\s?os|Management
                    excel|word|powerpoint|microsoft\s?office|
                    rest|graphql|api|soap|
                    flutter|android|ios|react\s?native|
                    pandas|numpy|scikit-learn|tensorflow|keras|pytorch|machine\s?learning|deep\s?learning|data\s?science|nlp|
                    cisco|tcp/ip|dns|vpn|firewall|networking|ccna|ccnp|it\s?support|troubleshooting|maintenance|diagnostic|sav|
                    rh|recrutement|formation|paie|gestion\s?du\s?personnel|grh|
                    communication|gestion\s?de\s?projet|leadership|organisation
                )\b
            """
        }
        self.standard_descriptions = {
           "experience": {
                "en": (
                    "experience\n"
                    "[Company Name], [Location] — [Job Title] ([Time Period])\n"
                    "Performed [responsibilities, tasks, or projects], utilizing skills in [technologies, tools, or methodologies].\n"
                    "Contributed to [team, project, or organizational goals], achieving [results or impact]."
                ),
                "fr": (
                    "Période : [Mois–Mois Année]  |  Entreprise : [Nom de l'entreprise], [Ville]\n"
                    "Poste , Stagiaire au sein du service [nom du service]\n"
                    "Tâches principales , suivi des activités quotidiennes, préparation de documents, aide à l’organisation du travail.\n"
                    "Responsabilités , respecter les délais, transmettre les informations, participer aux réunions internes.\n"
                    "Résultats , meilleure compréhension du fonctionnement d’un service et développement du sens des responsabilités."
                )
            },
            "education": {
                "en": (
    "education\n"
    "[Institution Name], [Location] — [Degree or Program] ([Time Period])\n"
    "- Field: [Field of Study or Focus Area]\n"
    "- Highlights: [Projects, Thesis, Courses, or Achievements]"
),

                "fr": (
                    "Diplômé en [Mois Année] avec un [Type de diplôme] en [Domaine d'études] de [Nom de l'université], [Ville]. "
                    "Obtenu un [Type de diplôme] en [Domaine d'études] en [Mois Année] de [Nom de l'université] à [Ville], [Pays]. "
                    "Mémoire de fin d'études portant sur [Sujet du mémoire], encadré par [Nom du professeur]. "
                    "Participation active à des projets universitaires et à des séminaires en lien avec [Thème ou domaine]."
                )
            }
        }

    def clean_text(self, text):
        """Clean text by removing special characters, keeping alphanumeric, accented characters, spaces, and newlines."""
        text = re.sub(r'[^\w\s\nÀ-ÿ]', '', text)
        return text

    def detect_language(self, text):
        """Detect the language of the input text, returning 'en' or 'fr'."""
        try:
            sample = text[:1000].lower()
            lang = detect(sample)
            return "fr" if lang == "fr" else "en" if lang == "en" else f"other({lang})"
        except Exception:
            return "unknown"

    def extract_sections_lines(self, text_cv, keyword_regex, num_lines=3):
        """Extract lines following a keyword match, stopping at the next section header."""
        lines = text_cv.split('\n')
        extracted = []
        section_headers = [self.keywords["experience"], self.keywords["education"], self.keywords["skills"]]
        combined_regex = '|'.join(f'({regex})' for regex in section_headers) 
        header_pattern = re.compile(combined_regex, re.IGNORECASE)

        for i, line in enumerate(lines):
            if re.search(keyword_regex, line, re.IGNORECASE):
                section_lines = [line.strip()]
                for j in range(i + 1, min(i + num_lines + 1, len(lines))):
                    if header_pattern.search(lines[j]):  
                        break
                    section_lines.append(lines[j].strip())
                last_index = min(i + len(section_lines) - 1, len(lines) - 1)
                extracted.append((line.strip(), section_lines, last_index))
        return extracted

    def embed(self, sentence_text):
        """Embed a sentence using the Transformer model and return the embedding tensor."""
        sentence = Sentence(sentence_text)
        self.embedding_model.embed(sentence)
        return sentence.embedding

    def score_similarity(self, base_sent, target_sent):
        """Calculate cosine similarity between two sentence embeddings."""
        return cosine_similarity(base_sent.unsqueeze(0), target_sent.unsqueeze(0), dim=1).item()

    def expand_lines(self, initial_lines, reference_text, last_index):
        """Expand initial lines by adding triplets of lines that improve similarity, avoiding duplicates."""
        current_text = ' '.join(initial_lines)
        current_score = self.score_similarity(self.embed(reference_text), self.embed(current_text))
        expanded_lines = list(initial_lines)
        lines = self.text_cv.split('\n')
        seen_lines = set(initial_lines)
        start_index = last_index + 1
        print(f"Starting expansion from index {start_index}")
        for i in range(start_index, len(lines) - 2, 1): 
            if i + 2 < len(lines): 
                next_line1 = lines[i].strip()
                next_line2 = lines[i + 1].strip()
                next_line3 = lines[i + 2].strip()
                # Skip if any line is already in seen_lines
                if any(line in seen_lines for line in (next_line1, next_line2, next_line3)):
                    print(f"Skipping triplet due to duplicate lines: ('{next_line1}', '{next_line2}', '{next_line3}')")
                    continue
                new_text = current_text + " " + next_line1 + " " + next_line2 + " " + next_line3
                new_score = self.score_similarity(self.embed(reference_text), self.embed(new_text))
                print(f"New score for triplet ('{next_line1}', '{next_line2}', '{next_line3}'): {new_score}")
                print(f"Current score: {current_score}")
                if new_score >= current_score:
                    expanded_lines.append(next_line1)
                    expanded_lines.append(next_line2)
                    expanded_lines.append(next_line3)
                    seen_lines.update([next_line1, next_line2, next_line3])
                    current_text = new_text
                    current_score = new_score
                else:
                    break
            else:
                break  
        return expanded_lines

    def extract_skills(self):
        """Extract skills from the CV text using the skills regex pattern."""
        skills_keywords = self.keywords['skills']
        pattern = re.compile(skills_keywords, re.IGNORECASE | re.VERBOSE)
        lines = self.text_cv.split('\n')
        skills = []
        for line in lines:
            matches = pattern.findall(line)
            skills.extend(matches)
        return list(set(skills))  
 
    # def extract_text_from_pdf(self):
    #     """Extraire texte via pytesseract, puis compléter avec lignes manquantes détectées par pdfplumber."""
    #     with pdfplumber.open(self.pdf_path) as pdf:
    #         for page in pdf.pages:
    #             if page.extract_text():
    #                 scanned= False  # Texte détecté → pas un PDF scanné
    #     scanned= True  # Aucune page avec du texte → probablement un PDF scanné
        
    # # Step 1 : Extraction via pytesseract
    #     if scanned:
    #        try:
    #            pages = convert_from_path(self.pdf_path)
    #            tesseract_text = ""
    #            for page in pages:
    #               tesseract_text += pytesseract.image_to_string(page, lang='eng+fra') + "\n"
    #        except Exception as e:
    #            print(f"Erreur d’extraction avec Tesseract : {e}")
    #            tesseract_text = ""

    # # Step 2 : Extraction via pdfplumber
    #     else:
    #         try:
    #             with pdfplumber.open(self.pdf_path) as pdf:
    #                 pdfplumber_text = "".join(page.extract_text() or "" for page in pdf.pages)
    #         except Exception as e:
    #             print(f"Erreur d’extraction avec pdfplumber : {e}")
    #             pdfplumber_text = ""

    # # Step 3 : Ajouter lignes manquantes de pdfplumber
    #     tesseract_lines = set(line.strip() for line in tesseract_text.splitlines() if line.strip())
    #     additional_lines = []

    #     for line in pdfplumber_text.splitlines():
    #        clean_line = line.strip()
    #        if clean_line and clean_line not in tesseract_lines:
    #           additional_lines.append(clean_line)

    #     if additional_lines:
    #        print(f"{len(additional_lines)} lignes manquantes ajoutées à la fin du texte.")
    #        tesseract_text += "\n".join(additional_lines)

    # # Step 4 : Correction facultative avec TextBlob
    #     try:
    #         language = self.detect_language(tesseract_text)
    #         if language in ["en", "fr"]:
    #            blob = TextBlob(tesseract_text)
    #            corrected_text = str(blob.correct())
    #         else:
    #            corrected_text = tesseract_text
    #     except Exception as e:
    #         print(f"Erreur lors de la correction du texte : {e}")
    #         corrected_text = tesseract_text

    #     return corrected_text
    # def is_scanned_pdf(self, max_pages_to_check=3):
    #  """Détecter si le PDF est probablement scanné (aucun texte extrait sur les premières pages)."""
    #  try:
    #     with pdfplumber.open(self.pdf_path) as pdf:
    #         for i, page in enumerate(pdf.pages[:max_pages_to_check]):
    #             if page.extract_text():
    #                 return False
    #     return True
    #  except Exception as e:
    #     print(f"Erreur lors de la détection du type de PDF : {e}")
    #     return True  # Par défaut, on suppose scanné en cas d’erreur

    def is_scanned_pdf(self, max_pages_to_check=3):
     """Détecter si le PDF est probablement scanné (aucun texte extrait sur les premières pages)."""
     try:
        with pdfplumber.open(self.pdf_path) as pdf:
            for i, page in enumerate(pdf.pages[:max_pages_to_check]):
                if page.extract_text():
                    return False
        return True
     except Exception as e:
        print(f"Erreur lors de la détection du type de PDF : {e}")
        return True  # Par défaut, on suppose scanné en cas d’erreur

    def extract_text_from_pdf(self):
        """Toujours convertir en image pour détecter les zones précises via LayoutParser."""
        text = ""
        try:
            print("Conversion du PDF en images...")
            pages = convert_from_path(self.pdf_path, dpi=300)

            print("Chargement du modèle LayoutParser...")
            model = lp.Detectron2LayoutModel('lp://PubLayNet/faster_rcnn_R_50_FPN_3x/config')

            for page_num, page_img in enumerate(pages):
                print(f"Traitement de la page {page_num + 1}...")
                layout = model.detect(page_img)

                # Trier les blocs pour respecter l’ordre visuel de lecture
                blocks = sorted(layout, key=lambda b: (b.block.y_1, b.block.x_1))

                for block in blocks:
                    # Extraire chaque segment d'image
                    segment = page_img.crop(block.coordinates)
                    block_text = pytesseract.image_to_string(segment, config='--psm 6', lang='eng+fra')
                    text += block_text.strip() + "\n\n"

        except Exception as e:
            print(f"Erreur lors de l’extraction : {e}")
            text = ""

        # Correction optionnelle
        try:
            language = self.detect_language(text)
            if language in ["en", "fr"]:
                blob = TextBlob(text)
                text = str(blob.correct())
        except Exception as e:
            print(f"Erreur lors de la correction grammaticale : {e}")

        return text.strip()

    # def extract_text_from_pdf(self):
    #     """Extract text from PDF using pytesseract (OCR) and pdfplumber, then correct with TextBlob."""
    #     try:
    #         pages = convert_from_path(self.pdf_path)
    #         tesseract_text = ""
    #         for page in pages:
    #             tesseract_text += pytesseract.image_to_string(page, lang='eng+fra') + "\n"
    #     except Exception as e:
    #         print(f"Error with pytesseract extraction: {e}")
    #         tesseract_text = ""

    #     try:
    #         with pdfplumber.open(self.pdf_path) as pdf:
    #             pdfplumber_text = "".join(page.extract_text() or "" for page in pdf.pages)
    #     except Exception as e:
    #         print(f"Error with pdfplumber extraction: {e}")
    #         pdfplumber_text = ""

    #     combined_text = tesseract_text if tesseract_text else pdfplumber_text
    #     if tesseract_text and pdfplumber_text:
    #         for section in ["experience", "education", "skills"]:
    #             tesseract_sections = self.extract_sections_lines(tesseract_text, self.keywords[section])
    #             if not tesseract_sections:
    #                 pdfplumber_sections = self.extract_sections_lines(pdfplumber_text, self.keywords[section])
    #                 if pdfplumber_sections:
    #                     combined_text += "\n" + "\n".join(line for _, lines in pdfplumber_sections for line in lines)

    #     language = self.detect_language(combined_text)
    #     blob = TextBlob(combined_text)
    #     try:
    #             corrected_text = str(blob.correct())
    #     except Exception as e:
    #         print(f"Error correcting text with TextBlob: {e}")
    #         corrected_text = combined_text

    #     return corrected_text
    def score_cv(self, offre_text, text_cv):
        """Compare l'offre d'emploi au texte du CV et retourne un score de similarité sémantique (0 à 1)."""
        try:
            offer_embedding = self.embed(offre_text)
            cv_embedding = self.embed(text_cv)
            score = self.score_similarity(cv_embedding, offer_embedding)
            return round(score, 4)
        except Exception as e:
            print(f"Erreur lors du calcul de similarité : {e}")
            return 0.0
    def analyze_cv_against_offer(self, offer: dict) -> tuple[str, float]:
        """Traite un CV et retourne le contenu structuré avec un score de similarité par rapport à une offre."""
        try:
            extracted_text = self.extract_text_from_pdf()
            self.text_cv = extracted_text
        except Exception as e:
            print(f"[Erreur] Extraction du texte : {e}")
            return "", 0.0

        cleaned_text = self.clean_text(extracted_text)
        language = self.detect_language(cleaned_text)

        organized_output = []

    # --- EXPÉRIENCE ---
        organized_output.append("=== Experience ===")
        experience_sections = self.extract_sections_lines(cleaned_text, self.keywords["experience"])
        if experience_sections:
           ref_text = self.standard_descriptions["experience"].get(language, self.standard_descriptions["experience"]["en"])
           scored_sections = []
           for header, lines, last_index in experience_sections:
               section_text = " ".join(lines)
               score = self.score_cv(section_text, ref_text)
               scored_sections.append((score, lines, last_index))

           best_section = max(scored_sections, key=lambda x: x[0])
           _, initial_lines, last_index = best_section
           expanded = self.expand_lines(initial_lines, ref_text, last_index)

           unique = list(dict.fromkeys(line.strip() for line in expanded if line.strip()))
           organized_output.extend(unique)
        else:
           organized_output.append("No experience sections found.")

    # --- ÉDUCATION ---
        organized_output.append("=== Education ===")
        education_sections = self.extract_sections_lines(cleaned_text, self.keywords["education"])
        if education_sections:
           ref_text = self.standard_descriptions["education"].get(language, self.standard_descriptions["education"]["en"])
           scored_sections = []
           for header, lines, last_index in education_sections:
              section_text = " ".join(lines)
              score = self.score_cv(section_text, ref_text)
              scored_sections.append((score, lines, last_index))

           best_section = max(scored_sections, key=lambda x: x[0])
           _, initial_lines, last_index = best_section
           expanded = self.expand_lines(initial_lines, ref_text, last_index)

           unique = list(dict.fromkeys(line.strip() for line in expanded if line.strip()))
           organized_output.extend(unique)
        else:
           organized_output.append("No education sections found.")

    # --- COMPÉTENCES ---
        organized_output.append("=== Skills ===")
        skills = self.extract_skills()
        organized_output.append(", ".join(skills) if skills else "No skills found.")

    # --- SCORING ---
        final_text = "\n".join(organized_output)
        offer_text = "\n".join(offer.get("missions", []) + offer.get("required_skills", []) + offer.get("optional_skills", []))
        final_score = self.score_cv(offer_text, final_text)

        return final_text, final_score


def main():
    pdf_path = r"C:/Users/OUMAIMA/Desktop/cv_oumeymaFinal.pdf"
    offer = {
        "title": "Développeur Mobile Flutter",
        "company": "DigitalWave Solutions",
        "location": "Tunis, Tunisie (Hybride)",
        "contract": "CDI",
        "experience": "1 à 3 ans",
        "salary": "À négocier selon profil",
        "start_date": "Dès que possible",
        "missions": [
            "Concevoir et développer des applications mobiles multiplateformes avec Flutter (iOS & Android).",
            "Participer à l’analyse des besoins techniques et fonctionnels.",
            "Intégrer des API RESTful et gérer les états via Provider, Bloc ou Riverpod.",
            "Assurer la qualité du code, les tests unitaires et la documentation.",
            "Collaborer avec les équipes backend, UI/UX et produit."
        ],
        "required_skills": [
            "Maîtrise de Flutter et Dart.",
            "Connaissance de l’architecture MVC ou MVVM.",
            "Intégration d’API REST et gestion de JSON.",
            "Utilisation de bases de données locales comme SQLite ou Hive.",
            "Connaissance de Git, GitHub/GitLab."
        ],
        "optional_skills": [
            "Expérience avec Firebase (Auth, Firestore, Storage).",
            "Connaissances en déploiement sur le Play Store / App Store.",
            "Notions en CI/CD pour mobile.",
            "Expérience avec des packages populaires : Dio, GetX, etc."
        ]
    }

    processor = PdfToText(pdf_path, "")
    structured_text, score = processor.analyze_cv_against_offer(offer)

    print("\n=== CV STRUCTURÉ ===\n")
    print(structured_text)
    print(f"\n=== SCORE DE MATCH AVEC L'OFFRE : {score}")



if __name__ == "__main__":
    main()